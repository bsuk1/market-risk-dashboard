import streamlit as st
import joblib
import yfinance as yf
import pandas as pd
import numpy as np
import requests # NewsAPI í˜¸ì¶œìš©
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch.nn.functional as F
import os # ë¹„ë°€(Secrets) ì ‘ê·¼ìš©

# --- 1. í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="í€€íŠ¸ ë¦¬ìŠ¤í¬ & ìŠ¤í¬ë¦¬ë„ˆ", layout="wide")
st.title("ğŸ“ˆ í€€íŠ¸ ë¦¬ìŠ¤í¬ ëŒ€ì‹œë³´ë“œ & S&P 500 ìŠ¤í¬ë¦¬ë„ˆ")

# --- 2. ë©”ì¸ íƒ­(Tabs) ìƒì„± ---
tab1, tab2 = st.tabs(["ì‹¤ì‹œê°„ ì‹œì¥ ë¦¬ìŠ¤í¬", "ğŸ“ˆ S&P 500 í€€íŠ¸ ìŠ¤í¬ë¦¬ë„ˆ"])


# ==============================================================================
# --- íƒ­ 1: ì‹¤ì‹œê°„ ì‹œì¥ ë¦¬ìŠ¤í¬ ---
# ==============================================================================
with tab1:
    st.header("ì‹¤ì‹œê°„ ì‹œì¥ ë¦¬ìŠ¤í¬ (Macro)")
    
    # --- (ìºì‹œ!) ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ëŠ” ì•± ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤. ---
    @st.cache_resource
    def load_models():
        try:
            model = joblib.load('crash_model_v1.pkl')
            scaler = joblib.load('scaler_v1.pkl')
            
            # FinBERT ëª¨ë¸ ë¡œë“œ (yiyanghkust/finbert-tone)
            finbert_tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
            finbert_model = AutoModelForSequenceClassification.from_pretrained("yiyanghkust/finbert-tone")
            
            return model, scaler, finbert_tokenizer, finbert_model
        except FileNotFoundError:
            st.error("ì˜¤ë¥˜: 'crash_model_v1.pkl' ë˜ëŠ” 'scaler_v1.pkl' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.error("Codespaceì— ì´ íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return None, None, None, None
        except Exception as e:
            st.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None, None, None, None

    model, scaler, finbert_tokenizer, finbert_model = load_models()

    # --- FinBERT ê°ì„± ë¶„ì„ í•¨ìˆ˜ ---
    @st.cache_data
    def analyze_sentiment(headlines):
        if not headlines:
            return 0.0
        device = torch.device("cpu")
        finbert_model.to(device)
        inputs = finbert_tokenizer(headlines, padding=True, truncation=True, return_tensors="pt", max_length=512)
        inputs = inputs.to(device) 
        with torch.no_grad():
            outputs = finbert_model(**inputs)
        probabilities = F.softmax(outputs.logits, dim=1)
        negative_scores = probabilities[:, 0].cpu().numpy()
        return np.mean(negative_scores)

    # --- ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ (API í˜¸ì¶œ) ---
    @st.cache_data(ttl=600)
    def get_market_data():
        """VIXì™€ 10Y-3M ê¸ˆë¦¬ ìŠ¤í”„ë ˆë“œì˜ ìµœì‹  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (ë”ìš± ì•ˆì „í•œ ë²„ì „)"""
        try:
            vix_series = yf.download('^VIX', period='10d')['Close']
            vix = float(vix_series.dropna().iloc[-1])
            
            tnx_series = yf.download('^TNX', period='10d')['Close']
            irx_series = yf.download('^IRX', period='10d')['Close']
            tnx = float(tnx_series.dropna().iloc[-1])
            irx = float(irx_series.dropna().iloc[-1])
            
            spread = tnx - irx
            return vix, spread
        except Exception as e:
            st.error(f"[íƒ­ 1] yfinance ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return None, None

    @st.cache_data(ttl=600)
    def get_news_headlines():
        """NewsAPIë¡œ ìµœì‹  ê¸ˆìœµ í—¤ë“œë¼ì¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            API_KEY = os.environ.get('NEWS_API_KEY') # (Secret ì´ë¦„ í™•ì¸!)
            if not API_KEY:
                st.error("NewsAPI í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (Codespace Secrets: NEWS_API_KEY)")
                return []
            url = (f'https://newsapi.org/v2/top-headlines?'
                   'sources=bloomberg,financial-post,the-wall-street-journal,reuters'
                   '&language=en'
                   f'&apiKey={API_KEY}')
            response = requests.get(url)
            data = response.json()
            if data['status'] == 'ok':
                headlines = [article['title'] for article in data['articles']]
                return headlines
            else:
                st.error(f"NewsAPI ì˜¤ë¥˜: {data.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                return []
        except Exception as e:
            st.error(f"NewsAPI ìš”ì²­ ì˜¤ë¥˜: {e}")
            return []

    # --- ë©”ì¸ ëŒ€ì‹œë³´ë“œ ë¡œì§ (íƒ­ 1) ---
    if model and scaler and finbert_model:
        vix_latest, spread_latest = get_market_data()
        headlines_latest = get_news_headlines()
        
        if vix_latest is not None and spread_latest is not None:
            input_data = np.array([[vix_latest, spread_latest]])
            input_scaled = scaler.transform(input_data)
            crash_probability = model.predict_proba(input_scaled)[0][1]
            crash_prob_percent = crash_probability * 100
            
            negative_sentiment = analyze_sentiment(headlines_latest)
            negative_sentiment_percent = negative_sentiment * 100

            st.subheader("ì‹¤ì‹œê°„ ë¦¬ìŠ¤í¬ ì§€í‘œ")
            col1, col2 = st.columns(2)
            with col1:
                st.metric(
                    label="ëª¨ë¸ ê¸°ë°˜ í­ë½ í™•ë¥  (VIX, ê¸ˆë¦¬)",
                    value=f"{crash_prob_percent:.2f} %",
                    delta=f"{crash_prob_percent - 30:.2f} % (vs. ê¸°ì¤€ 30%)",
                    delta_color="inverse"
                )
            with col2:
                st.metric(
                    label="ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë¶€ì • ê°ì„± (FinBERT)",
                    value=f"{negative_sentiment_percent:.2f} %",
                    delta=f"{negative_sentiment_percent - 50:.2f} % (vs. ê¸°ì¤€ 50%)",
                    delta_color="inverse"
                )

            st.subheader("ìµœì‹  ì…ë ¥ ë°ì´í„°")
            col1, col2 = st.columns(2)
            col1.info(f"**ìµœì‹  VIX:** {vix_latest:.2f}")
            col2.info(f"**ìµœì‹  10Y-3M ìŠ¤í”„ë ˆë“œ:** {spread_latest:.2f}")

            st.subheader("ìµœì‹  ë¶„ì„ í—¤ë“œë¼ì¸ (NewsAPI)")
            if headlines_latest:
                st.dataframe(pd.DataFrame(headlines_latest, columns=["Headline"]), use_container_width=True)
            else:
                st.warning("í—¤ë“œë¼ì¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        st.error("ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ [íƒ­ 1]ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


# ==============================================================================
# --- íƒ­ 2: S&P 500 í€€íŠ¸ ìŠ¤í¬ë¦¬ë„ˆ ---
# (*** ì´ ë¶€ë¶„ì´ "Top 50" ê¸°ëŠ¥ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œë˜ì—ˆìŠµë‹ˆë‹¤ ***)
# ==============================================================================
with tab2:
    st.header("S&P 500 í€€íŠ¸ ìŠ¤í¬ë¦¬ë„ˆ (Micro)")
    st.write('S&P 500 ê¸°ì—… ì¤‘ "ê¸°ë³¸ê¸°(Fundamentals)ê°€ ì¢‹ì€" ì£¼ì‹ì„ í•„í„°ë§í•˜ê³ , "í˜„ì¬ ë‰´ìŠ¤ ê°ì„±"ì„ ë¶„ì„í•©ë‹ˆë‹¤.')
    st.write('**ì‹ ê·œ ê¸°ëŠ¥:** ì„¹í„°ë³„ í•„í„°ë§ í›„, ì§€ì •í•œ ê¸°ì¤€ì˜ **Top 50** ê¸°ì—…ë§Œ í‘œì‹œí•©ë‹ˆë‹¤.')

    # --- (FinBERT ëª¨ë¸ ë¡œë“œ - íƒ­ 1ì—ì„œ ë¡œë“œëœ ê²ƒì„ ì¬ì‚¬ìš©) ---
    if not finbert_model or not finbert_tokenizer:
        st.error("FinBERT ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. [ì‹¤ì‹œê°„ ì‹œì¥ ë¦¬ìŠ¤í¬] íƒ­ì„ ë¨¼ì € ë°©ë¬¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    # --- ìŠ¤í¬ë¦¬ë„ˆ í•¨ìˆ˜ (íƒ­ 2 ì „ìš©) ---
    
    @st.cache_data(ttl=86400) # 24ì‹œê°„ ìºì‹œ
    def get_sp500_tickers():
        """ìœ„í‚¤í”¼ë””ì•„ì—ì„œ S&P 500 í‹°ì»¤ ëª©ë¡ì„ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤. (User-Agent ìˆ˜ì •ë¨)"""
        url = 'https://en.wikipedia.org/wiki/List_of_S&P_500_companies'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status() # ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ ì—¬ê¸°ì„œ ë©ˆì¶¤
        table = pd.read_html(response.text)
        tickers = table[0]['Symbol'].tolist()
        return [ticker.replace('.', '-') for ticker in tickers]

    @st.cache_data(ttl=86400) # 24ì‹œê°„ ìºì‹œ
    def get_fundamental_data(tickers):
        """í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ yfinance.infoë¡œ ê¸°ë³¸ê¸° ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        data = []
        progress_bar = st.progress(0, text="S&P 500 ì¬ë¬´ ë°ì´í„° ë¡œë“œ ì¤‘... (1/500)")
        
        total_tickers = len(tickers)
        for i, ticker in enumerate(tickers):
            try:
                tk = yf.Ticker(ticker)
                info = tk.info
                data.append({
                    'Ticker': ticker,
                    'Company Name': info.get('shortName'),
                    'Sector': info.get('sector'),
                    'Market Cap (Billions)': info.get('marketCap', 0) / 1e9,
                    'P/E (Forward)': info.get('forwardPE'),
                    'P/B (Price to Book)': info.get('priceToBook'),
                    'ROE (Return on Equity)': info.get('returnOnEquity'),
                    'Revenue Growth (YoY)': info.get('revenueGrowth'),
                    'Debt/Equity': info.get('debtToEquity')
                })
            except Exception as e:
                pass 
            progress_bar.progress(
                (i + 1) / total_tickers, 
                text=f"S&P 500 ì¬ë¬´ ë°ì´í„° ë¡œë“œ ì¤‘... ({ticker}: {i+1}/{total_tickers})"
            )
        progress_bar.empty() 
        # (ì¤‘ìš”!) Sectorê°€ ì—†ëŠ” ë°ì´í„°ëŠ” ì œê±°
        return pd.DataFrame(data).dropna(subset=['P/E (Forward)', 'P/B (Price to Book)', 'Company Name', 'Sector'])

    # --- (ì‹ ê·œ!) Phase 2: ê°œë³„ ì£¼ì‹ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ í•¨ìˆ˜ ---
    @st.cache_data(ttl=3600) # 1ì‹œê°„(3600ì´ˆ) ìºì‹œ
    def get_news_for_stock(company_name):
        """NewsAPIë¡œ íŠ¹ì • íšŒì‚¬ì˜ í—¤ë“œë¼ì¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            API_KEY = os.environ.get('NEWS_API_KEY') # (Secret ì´ë¦„ í™•ì¸!)
            if not API_KEY:
                st.error("NewsAPI í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
            url = (f'https://newsapi.org/v2/everything?'
                   f'q="{company_name}"' 
                   '&language=en&pageSize=20'
                   f'&apiKey={API_KEY}')
            response = requests.get(url)
            data = response.json()
            if data['status'] == 'ok':
                return [article['title'] for article in data['articles']]
            else:
                return []
        except Exception as e:
            st.warning(f"{company_name} ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    @st.cache_data(ttl=3600) # 1ì‹œê°„(3600ì´ˆ) ìºì‹œ
    def get_sentiment_for_stocks(df_filtered):
        """í•„í„°ë§ëœ DataFrameì„ ë°›ì•„ ê°œë³„ ë‰´ìŠ¤ ê°ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤. (iterrows ìˆ˜ì •)"""
        sentiment_scores = []
        
        progress_bar = st.progress(0, text="ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì¤‘... (1/?)")
        total_stocks = len(df_filtered)
        
        # --- (ìˆ˜ì •ë¨!) .itertuples() ëŒ€ì‹  .iterrows() ì‚¬ìš© ---
        for i, (index, row) in enumerate(df_filtered.iterrows()):
            
            # .iterrows()ëŠ” rowê°€ Series ê°ì²´ì´ë¯€ë¡œ, ë”•ì…”ë„ˆë¦¬ì²˜ëŸ¼ []ë¡œ ì ‘ê·¼í•©ë‹ˆë‹¤.
            # ì´ë¦„ ë³€í™˜(mangling)ì´ ì—†ì–´ ì•ˆì „í•©ë‹ˆë‹¤.
            company_name = row['Company Name'] 
            
            progress_bar.progress(
                (i + 1) / total_stocks, 
                text=f"ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì¤‘... ({company_name}: {i+1}/{total_stocks})"
            )
            
            headlines = get_news_for_stock(company_name)
            if headlines:
                score = analyze_sentiment(headlines)
                sentiment_scores.append(score)
            else:
                sentiment_scores.append(np.nan) 
                
        progress_bar.empty()
        # (ê²°ê³¼ë¥¼ í• ë‹¹í•  ë•Œ .copy()ë¥¼ ì‚¬ìš©í•´ì•¼ ê²½ê³ ê°€ ëœ¨ì§€ ì•ŠìŠµë‹ˆë‹¤)
        df_result = df_filtered.copy()
        df_result['Negative Score'] = sentiment_scores
        return df_result.dropna(subset=['Negative Score'])

    # --- (ì‹ ê·œ!) ì„¹í„°ë³„ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ ---
    DEFAULT_SECTOR_PARAMS = {
        'Technology':       {'pe_max': 40.0, 'pb_max': 10.0, 'roe_min': 0.15, 'revg_min': 0.10},
        'Healthcare':       {'pe_max': 25.0, 'pb_max': 7.0,  'roe_min': 0.10, 'revg_min': 0.05},
        'Financials':       {'pe_max': 20.0, 'pb_max': 2.0,  'roe_min': 0.08, 'revg_min': 0.03},
        'Consumer Cyclical':{'pe_max': 25.0, 'pb_max': 5.0,  'roe_min': 0.12, 'revg_min': 0.05},
        'Consumer Defensive':{'pe_max': 20.0, 'pb_max': 3.5,  'roe_min': 0.10, 'revg_min': 0.03},
        'Industrials':      {'pe_max': 22.0, 'pb_max': 4.0,  'roe_min': 0.10, 'revg_min': 0.05},
        'Real Estate':      {'pe_max': 25.0, 'pb_max': 3.0,  'roe_min': 0.05, 'revg_min': 0.03},
        'Utilities':        {'pe_max': 20.0, 'pb_max': 2.5,  'roe_min': 0.08, 'revg_min': 0.02},
        'Communication Services': {'pe_max': 28.0, 'pb_max': 5.0, 'roe_min': 0.10, 'revg_min': 0.05},
        'Energy':           {'pe_max': 15.0, 'pb_max': 2.5,  'roe_min': 0.05, 'revg_min': 0.03},
        'Basic Materials':  {'pe_max': 20.0, 'pb_max': 3.0,  'roe_min': 0.08, 'revg_min': 0.03},
        'Other':            {'pe_max': 25.0, 'pb_max': 5.0,  'roe_min': 0.10, 'revg_min': 0.05}
    }
    
    # --- ìŠ¤í¬ë¦¬ë„ˆ UI (íƒ­ 2) ---
    
    if 'df_fundamentals' not in st.session_state:
        st.info("S&P 500 ì „ì²´ ê¸°ì—…ì˜ ì¬ë¬´ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.")
        if st.button("S&P 500 ë°ì´í„° ë¡œë“œí•˜ê¸° (ìµœì´ˆ 1íšŒ 5~10ë¶„ ì†Œìš”)", key='load_sp500'):
            with st.spinner("Wikipediaì—ì„œ S&P 500 ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì¤‘..."):
                tickers = get_sp500_tickers()
            df_fundamentals = get_fundamental_data(tickers)
            st.session_state['df_fundamentals'] = df_fundamentals
            
            # (ì‹ ê·œ!) ì„¹í„° íŒŒë¼ë¯¸í„°ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì´ˆê¸°í™”
            st.session_state['sector_params'] = DEFAULT_SECTOR_PARAMS.copy()
            st.rerun() 
    
    if 'df_fundamentals' in st.session_state:
        df_full = st.session_state['df_fundamentals']
        
        # (ì‹ ê·œ!) ì„¹í„° íŒŒë¼ë¯¸í„° ì„¤ì • UI (ì‚¬ì´ë“œë°” ëŒ€ì‹  ë©”ì¸ íƒ­ì—)
        st.sidebar.info("ì„¹í„°ë³„ ìƒì„¸ í•„í„°ëŠ” 'í€€íŠ¸ ìŠ¤í¬ë¦¬ë„ˆ' íƒ­ ë©”ì¸ í™”ë©´ì—ì„œ ì„¤ì •í•˜ì„¸ìš”.")
        
        with st.expander("ğŸ“ˆ (Phase 1) ì„¹í„°ë³„ í€ë”ë©˜íƒˆ í•„í„° ì„¤ì •"):
            # (ë°ì´í„°ì— ìˆëŠ” ì„¹í„° ëª©ë¡ + 'Other' ì¶”ê°€)
            available_sectors = list(df_full['Sector'].unique())
            if 'Other' not in available_sectors:
                available_sectors.append('Other')
                
            # (st.tabsë¥¼ ì‚¬ìš©í•´ ì„¹í„°ë³„ë¡œ UI ë¶„ë¦¬)
            sector_tabs = st.tabs(available_sectors)
            
            for i, sector_name in enumerate(available_sectors):
                with sector_tabs[i]:
                    # (ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ íŒŒë¼ë¯¸í„° ê°’ì„ ê°€ì ¸ì˜´)
                    if sector_name not in st.session_state.sector_params:
                        st.session_state.sector_params[sector_name] = DEFAULT_SECTOR_PARAMS['Other'] # ê¸°ë³¸ê°’
                        
                    params = st.session_state.sector_params[sector_name]
                    
                    c1, c2 = st.columns(2)
                    c3, c4 = st.columns(2)
                    
                    # (ê° ìŠ¬ë¼ì´ë”ê°€ st.session_stateë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ë„ë¡ key ì„¤ì •)
                    params['pe_max'] = c1.slider('ìµœëŒ€ P/E', 5.0, 100.0, params['pe_max'], 1.0, key=f'pe_{sector_name}')
                    params['pb_max'] = c2.slider('ìµœëŒ€ P/B', 0.1, 20.0, params['pb_max'], 0.1, key=f'pb_{sector_name}')
                    params['roe_min'] = c3.slider('ìµœì†Œ ROE', 0.0, 1.0, params['roe_min'], 0.01, format="%.2f", key=f'roe_{sector_name}')
                    params['revg_min'] = c4.slider('ìµœì†Œ ë§¤ì¶œ ì„±ì¥ë¥ ', 0.0, 1.0, params['revg_min'], 0.01, format="%.2f", key=f'revg_{sector_name}')

        # --- (ì‹ ê·œ!) í•„í„°ë§ ë¡œì§ (ì„¹í„°ë³„ ì ìš©) ---
        filtered_dfs = []
        for sector_name, params in st.session_state.sector_params.items():
            sector_df = df_full[df_full['Sector'] == sector_name]
            
            if not sector_df.empty:
                sector_filtered = sector_df[
                    (sector_df['P/E (Forward)'] <= params['pe_max']) &
                    (sector_df['P/B (Price to Book)'] <= params['pb_max']) &
                    (sector_df['ROE (Return on Equity)'] >= params['roe_min']) &
                    (sector_df['Revenue Growth (YoY)'] >= params['revg_min'])
                ]
                filtered_dfs.append(sector_filtered)

        df_filtered = pd.concat(filtered_dfs).sort_index()
        
        # --- (ì‹ ê·œ!) Top 50 í•„í„°ë§ ë° ê²°ê³¼ í‘œì‹œ ---
        st.subheader(f"í•„í„°ë§ëœ 'ìš°ëŸ‰ì£¼' í›„ë³´: ({len(df_filtered)} / {len(df_full)}ê°œ)")
        
        if not df_filtered.empty:
            # --- (ì‹ ê·œ!) Top 50 ì •ë ¬ ê¸°ì¤€ ì„ íƒ ---
            sort_by = st.selectbox(
                "Top 50 ì •ë ¬ ê¸°ì¤€ ì„ íƒ",
                ('P/E (Forward) (ë‚®ì€ ìˆœ)', 
                 'Market Cap (Billions) (ë†’ì€ ìˆœ)', 
                 'ROE (Return on Equity) (ë†’ì€ ìˆœ)'),
                key='sort_by_top50'
            )

            if sort_by == 'P/E (Forward) (ë‚®ì€ ìˆœ)':
                df_sorted = df_filtered.sort_values(by='P/E (Forward)', ascending=True)
            elif sort_by == 'Market Cap (Billions) (ë†’ì€ ìˆœ)':
                df_sorted = df_filtered.sort_values(by='Market Cap (Billions)', ascending=False)
            else: # ROE
                df_sorted = df_filtered.sort_values(by='ROE (Return on Equity)', ascending=False)
            
            df_top50 = df_sorted.head(50)
            
            st.dataframe(df_top50, use_container_width=True)
            
            st.divider()
            st.header("ğŸ“° (Phase 2) ë‰´ìŠ¤ ê°ì„± ë¶„ì„")
            
            # --- (ì‹ ê·œ!) Phase 2 ì‹¤í–‰ ë²„íŠ¼ (df_top50ì„ ì‚¬ìš©) ---
            if st.button(f"í•„í„°ë§ëœ Top {len(df_top50)}ê°œ ì£¼ì‹ ë‰´ìŠ¤ ê°ì„± ë¶„ì„í•˜ê¸°", key='run_sentiment'):
                if len(df_top50) == 0:
                    st.error("ë¶„ì„í•  ì£¼ì‹ì´ ì—†ìŠµë‹ˆë‹¤. Phase 1 í•„í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")
                else:
                    # (get_sentiment_for_stocks í•¨ìˆ˜ê°€ df_top50ì„ ë°›ë„ë¡ ìˆ˜ì •)
                    df_with_sentiment = get_sentiment_for_stocks(df_top50.copy())
                    st.session_state['df_sentiment'] = df_with_sentiment
        else:
            st.warning("Phase 1 í•„í„° ì¡°ê±´ì— ë§ëŠ” ì£¼ì‹ì´ ì—†ìŠµë‹ˆë‹¤.")


        # --- Phase 3: ìµœì¢… ê²°ê³¼ í‘œì‹œ ---
        if 'df_sentiment' in st.session_state:
            st.header("ğŸ¯ (Phase 3) ì €í‰ê°€ ê¸°íšŒ ëª©ë¡ (ìš°ëŸ‰ì£¼ + ë‚˜ìœ ë‰´ìŠ¤)")
            st.write("ê¸°ë³¸ê¸°ëŠ” ì¢‹ì§€ë§Œ(Phase 1), í˜„ì¬ ë‹¨ê¸°ì ìœ¼ë¡œ ë¶€ì •ì ì¸ ë‰´ìŠ¤(Phase 2)ê°€ ë§ì€ ì£¼ì‹ì…ë‹ˆë‹¤.")
            
            df_final = st.session_state['df_sentiment']
            st.dataframe(
                df_final.sort_values(by='Negative Score', ascending=False), 
                use_container_width=True,
                column_config={
                    "Negative Score": st.column_config.ProgressColumn(
                        "ë¶€ì • ë‰´ìŠ¤ ì ìˆ˜",
                        format="%.2f",
                        min_value=0.0,
                        max_value=1.0,
                    )
                }
            )

        if st.sidebar.button("ìºì‹œ ì§€ìš°ê³  ëª¨ë“  ë°ì´í„° ë‹¤ì‹œ ë¡œë“œ", key='clear_cache_sp500'):
            st.cache_data.clear() # ëª¨ë“  ìºì‹œ ì§€ìš°ê¸°
            # ì„¸ì…˜ ìƒíƒœë„ ëª¨ë‘ ì‚­ì œ
            for key in list(st.session_state.keys()):
                if key in ['df_fundamentals', 'df_sentiment', 'sector_params']:
                    del st.session_state[key]
            st.rerun()